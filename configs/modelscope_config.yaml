# ModelScope training configuration for GraphMER-SE
# Optimized for ModelScope environment with appropriate resource constraints

run:
  seed: 42
  epochs: 10
  gradient_accumulation_steps: 32  # Adjusted for ModelScope resources
  log_interval: 25
  eval_interval_steps: 1000
  save_interval_steps: 2500
  mixed_precision: false  # Safe default for compatibility
  deterministic: true

hardware:
  device: auto  # Will auto-detect GPU if available
  num_workers: 2   # Reduced for ModelScope environment
  pin_memory: true

model:
  hidden_size: 768  # Restored to baseline for proper foundation model
  num_layers: 12    # Restored to baseline for proper foundation model  
  num_heads: 12
  intermediate_size: 3072
  dropout: 0.1
  positional_encoding: alibi  # CPU-friendly
  norm: rmsnorm
  activation: swiglu
  hgat:
    enabled: true
    relation_bias: true
  use_rel_attention_bias: true  # Validated: 50% MNM improvement in ablation

optimizer:
  name: adamw
  lr: 0.0002  # Slightly higher for faster convergence in limited time
  weight_decay: 0.01
  betas: [0.9, 0.98]
  eps: 1e-08
  scheduler:
    name: cosine
    warmup_steps: 1000

training_data:
  max_seq_len: 512   # Restored to baseline for proper foundation model
  micro_batch_size: 1
  pack_sequences: true
  short_to_long_curriculum:
    enabled: true
    schedule:
      - {steps: 0, max_seq_len: 384}
      - {steps: 1000, max_seq_len: 512}

objectives:
  mlm:
    mask_prob: 0.15
    span_mask_identifiers: true
  mnm:
    mask_prob: 0.20
    type_consistent_negatives: 2
    hard_negatives: 1

encoding:
  leaves_per_anchor:
    positive: 2
    negatives: 2
  max_leaves_per_sequence: 8

regularizers:
  ontology_constraints:
    antisymmetry_weight: 0.2
    acyclicity_weight: 0.2
  contrastive:
    enabled: true
    temperature: 0.07

checkpointing:
  activation_checkpointing: true  # Reduce memory usage
  gradient_checkpointing: true

notes:
  - ModelScope config with FULL baseline dimensions (768/12/12/3072) - NOT reduced!
  - Fixed: training script now reads all model dimensions from config
  - Increased learning rate (0.0002) for faster convergence
  - Curriculum learning: 384â†’512 seq length ramp-up for training stability