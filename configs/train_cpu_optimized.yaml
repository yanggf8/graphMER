# Optimized CPU Training Configuration
# Based on benchmark: 12th Gen Intel i5-1240P (8 cores, 16 threads, 19GB RAM)
# Target: 10,000 steps in ~12-13 hours
# Benchmark: 4.55 seconds per effective step with grad_accum=4

run:
  seed: 42
  epochs: 5
  gradient_accumulation_steps: 8  # Effective batch size = 8
  log_interval: 10                # Log every 10 steps
  eval_interval_steps: 500        # Evaluate every 500 steps
  save_interval_steps: 1000       # Save checkpoint every 1000 steps
  mixed_precision: false          # CPU doesn't benefit from fp16
  deterministic: true

hardware:
  device: cpu
  num_workers: 4      # Good for 8-core CPU with hyperthreading
  pin_memory: false   # No GPU, so no benefit

model:
  hidden_size: 768
  num_layers: 12
  num_heads: 12
  intermediate_size: 3072
  dropout: 0.1
  positional_encoding: alibi  # CPU-friendly alternative to learned PE
  norm: rmsnorm
  activation: swiglu
  hgat:
    enabled: true
    relation_bias: true
  use_rel_attention_bias: true  # Validated: 50% MNM improvement

optimizer:
  name: adamw
  lr: 3.0e-4           # Higher LR for faster convergence on CPU
  weight_decay: 0.01
  betas: [0.9, 0.98]
  eps: 1e-8
  scheduler:
    name: cosine
    warmup_steps: 500   # Warm up over first 500 steps

training_data:
  max_seq_len: 128     # Reduced from 512 for 4x speedup (128² vs 512²)
  micro_batch_size: 1  # Small batch for CPU memory efficiency
  pack_sequences: true
  curriculum_learning:
    enabled: false     # Disabled for simplicity; use fixed seq_len

objectives:
  mlm:
    mask_prob: 0.15
    span_mask_identifiers: true
  mnm:
    mask_prob: 0.20
    type_consistent_negatives: 2
    hard_negatives: 1
  mlm_loss_weight: 1.0
  mnm_loss_weight: 1.0

encoding:
  leaves_per_anchor:
    positive: 2
    negatives: 2
  max_leaves_per_sequence: 8

regularizers:
  ontology_constraints:
    antisymmetry_weight: 0.2
    acyclicity_weight: 0.2
  contrastive:
    enabled: true
    temperature: 0.07

checkpointing:
  activation_checkpointing: true   # Save memory by recomputing activations
  gradient_checkpointing: true     # Trade compute for memory

notes:
  - "Optimized for 12th Gen Intel i5-1240P CPU"
  - "Sequence length reduced to 128 for 4x speedup"
  - "Expected: 10,000 steps in ~12-13 hours"
  - "Gradient accumulation compensates for small micro_batch_size"
  - "Checkpoints saved every 1000 steps for incremental evaluation"
  - "Run overnight for results by morning"

# Performance expectations:
#   - Time per step: ~4.5 seconds
#   - 100 steps: ~7.5 minutes
#   - 1000 steps: ~1.3 hours
#   - 10000 steps: ~12.6 hours
