# Training config for NVIDIA RTX 3050 (8GB VRAM)
# Optimized for local GPU training with FP16 mixed precision

run:
  seed: 1337
  epochs: 5
  gradient_accumulation_steps: 1  # Not needed with larger batch
  log_interval: 50
  eval_interval_steps: 500
  save_interval_steps: 1000
  mixed_precision: fp16  # Automatic mixed precision for RTX 3050
  deterministic: true

hardware:
  device: cuda
  num_workers: 4  # Adjust based on CPU cores available

model:
  hidden_size: 768
  num_layers: 12
  num_heads: 12
  intermediate_size: 3072
  dropout: 0.1
  positional_encoding: alibi
  norm: rmsnorm
  activation: swiglu
  hgat:
    enabled: true
    relation_bias: true
  use_rel_attention_bias: true  # Validated: 14.29% MNM improvement

optimizer:
  name: adamw
  lr: 3.0e-4
  weight_decay: 0.01
  betas: [0.9, 0.98]
  eps: 1e-8
  scheduler:
    name: cosine
    warmup_steps: 1500

training_data:
  max_seq_len: 768
  micro_batch_size: 32  # Can use larger batch than TPU (2 per core)
  pack_sequences: true
  short_to_long_curriculum:
    enabled: true
    schedule:
      - {steps: 0, max_seq_len: 512}
      - {steps: 15000, max_seq_len: 768}

objectives:
  mlm:
    mask_prob: 0.15
    span_mask_identifiers: true
  mnm:
    mask_prob: 0.20
    type_consistent_negatives: 2
    hard_negatives: 2

encoding:
  leaves_per_anchor:
    positive: 2
    negatives: 2
  max_leaves_per_sequence: 10

regularizers:
  ontology_constraints:
    antisymmetry_weight: 0.2
    acyclicity_weight: 0.2
  contrastive:
    enabled: true
    temperature: 0.07

checkpointing:
  activation_checkpointing: false  # Not needed with 8GB VRAM

notes:
  - "Optimized for NVIDIA RTX 3050 (8GB VRAM)"
  - "Uses FP16 mixed precision for faster training"
  - "Batch size 32 fits comfortably in 8GB VRAM"
  - "No session limits unlike Colab TPU"
  - "Expected: 5-8 steps/sec, 1000 steps in 2-3 minutes"
