#!/bin/bash
# GraphMER-SE Kaggle Deployment Script
# Creates a dataset package for upload to Kaggle

set -e  # Exit on error

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

echo -e "${BLUE}โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ${NC}"
echo -e "${BLUE}โ                                                              โ${NC}"
echo -e "${BLUE}โ         GraphMER-SE - Kaggle Dataset Builder                โ${NC}"
echo -e "${BLUE}โ                                                              โ${NC}"
echo -e "${BLUE}โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ${NC}"
echo ""

# Configuration
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(dirname "$SCRIPT_DIR")"
OUTPUT_DIR="$PROJECT_ROOT/kaggle_deploy"
DATASET_NAME="graphmer-kg"
ARCHIVE_NAME="graphmer_kaggle_dataset.zip"

echo -e "${YELLOW}๐ Configuration:${NC}"
echo "   Project Root: $PROJECT_ROOT"
echo "   Output Dir:   $OUTPUT_DIR"
echo "   Dataset Name: $DATASET_NAME"
echo ""

# Step 1: Clean previous build
echo -e "${YELLOW}๐งน Step 1/6: Cleaning previous build...${NC}"
if [ -d "$OUTPUT_DIR" ]; then
    rm -rf "$OUTPUT_DIR"
    echo "   โ Removed old deploy directory"
fi
mkdir -p "$OUTPUT_DIR"
echo "   โ Created fresh deploy directory"
echo ""

# Step 2: Create directory structure
echo -e "${YELLOW}๐ Step 2/6: Creating directory structure...${NC}"
mkdir -p "$OUTPUT_DIR/src"
mkdir -p "$OUTPUT_DIR/configs"
mkdir -p "$OUTPUT_DIR/scripts"
mkdir -p "$OUTPUT_DIR/docs/specs"
echo "   โ Directory structure created"
echo ""

# Step 3: Copy source code
echo -e "${YELLOW}๐ฆ Step 3/6: Copying source code...${NC}"
cp -r "$PROJECT_ROOT/src/"* "$OUTPUT_DIR/src/"
echo "   โ Copied src/"

cp -r "$PROJECT_ROOT/configs/"*.yaml "$OUTPUT_DIR/configs/"
echo "   โ Copied configs/"

# Copy essential scripts
for script in train.py eval.py validate_tpu_setup.py; do
    if [ -f "$PROJECT_ROOT/scripts/$script" ]; then
        cp "$PROJECT_ROOT/scripts/$script" "$OUTPUT_DIR/scripts/"
        echo "   โ Copied scripts/$script"
    fi
done

cp "$PROJECT_ROOT/docs/specs/ontology_spec.yaml" "$OUTPUT_DIR/docs/specs/"
echo "   โ Copied ontology spec"
echo ""

# Step 4: Copy and verify data
echo -e "${YELLOW}๐ Step 4/6: Copying and verifying data...${NC}"

# Check which dataset to use (prefer seed_python with 29k+ triples)
if [ -f "$PROJECT_ROOT/data/kg/seed_python.jsonl" ]; then
    TRIPLES_FILE="seed_python.jsonl"
    ENTITIES_FILE="seed_python.entities.jsonl"
    echo "   ๐ Using seed_python dataset (29,174 triples)"
elif [ -f "$PROJECT_ROOT/data/kg/enhanced_multilang.jsonl" ]; then
    TRIPLES_COUNT=$(wc -l < "$PROJECT_ROOT/data/kg/enhanced_multilang.jsonl")
    if [ "$TRIPLES_COUNT" -ge 20000 ]; then
        TRIPLES_FILE="enhanced_multilang.jsonl"
        ENTITIES_FILE="enhanced_multilang.entities.jsonl"
        echo "   ๐ Using enhanced_multilang dataset ($TRIPLES_COUNT triples)"
    else
        echo -e "${RED}   โ enhanced_multilang.jsonl has only $TRIPLES_COUNT triples${NC}"
        echo -e "${RED}   โ seed_python.jsonl not found (need 29k+ triples)${NC}"
        echo -e "${YELLOW}   ๐ก Recommendation: Use seed_python.jsonl if available${NC}"
        exit 1
    fi
else
    echo -e "${RED}   โ No data files found!${NC}"
    echo -e "${RED}   Expected: data/kg/seed_python.jsonl or enhanced_multilang.jsonl${NC}"
    exit 1
fi

# Copy data files (use consistent naming for Kaggle)
cp "$PROJECT_ROOT/data/kg/$TRIPLES_FILE" "$OUTPUT_DIR/enhanced_multilang.jsonl"
cp "$PROJECT_ROOT/data/kg/$ENTITIES_FILE" "$OUTPUT_DIR/enhanced_multilang.entities.jsonl"

# Verify triple count
TRIPLE_COUNT=$(wc -l < "$OUTPUT_DIR/enhanced_multilang.jsonl")
if [ "$TRIPLE_COUNT" -ge 20000 ]; then
    echo -e "   ${GREEN}โ Data verified: $TRIPLE_COUNT triples (โฅ20,000)${NC}"
else
    echo -e "   ${RED}โ Insufficient data: $TRIPLE_COUNT triples (need โฅ20,000)${NC}"
    exit 1
fi

# Check file sizes
TRIPLES_SIZE=$(du -h "$OUTPUT_DIR/enhanced_multilang.jsonl" | cut -f1)
ENTITIES_SIZE=$(du -h "$OUTPUT_DIR/enhanced_multilang.entities.jsonl" | cut -f1)
echo "   ๐ Triples file: $TRIPLES_SIZE"
echo "   ๐ Entities file: $ENTITIES_SIZE"
echo ""

# Step 5: Create metadata files
echo -e "${YELLOW}๐ Step 5/6: Creating metadata files...${NC}"

# Create dataset-metadata.json for Kaggle
cat > "$OUTPUT_DIR/dataset-metadata.json" <<EOF
{
  "title": "GraphMER Knowledge Graph Dataset",
  "id": "YOUR_USERNAME/$DATASET_NAME",
  "licenses": [
    {
      "name": "CC0-1.0"
    }
  ],
  "keywords": [
    "knowledge graph",
    "software engineering",
    "code analysis",
    "graph neural networks",
    "python",
    "java"
  ],
  "description": "GraphMER-SE Knowledge Graph dataset with 30,826+ triples extracted from Python and Java code. Includes ontology validation and 99.10% quality score.",
  "resources": [
    {
      "path": "enhanced_multilang.jsonl",
      "description": "Knowledge graph triples (30,826+ entries)"
    },
    {
      "path": "enhanced_multilang.entities.jsonl",
      "description": "Entity metadata"
    },
    {
      "path": "src/",
      "description": "Source code for GraphMER model"
    },
    {
      "path": "configs/",
      "description": "Training configurations"
    },
    {
      "path": "scripts/",
      "description": "Training and evaluation scripts"
    },
    {
      "path": "docs/specs/ontology_spec.yaml",
      "description": "Ontology specification"
    }
  ]
}
EOF
echo "   โ Created dataset-metadata.json"

# Create README for dataset
cat > "$OUTPUT_DIR/README.md" <<EOF
# GraphMER Knowledge Graph Dataset

**Version:** 1.0
**Created:** $(date +%Y-%m-%d)
**Triples:** $TRIPLE_COUNT
**Quality:** 99.10% validation rate

## Overview

This dataset contains a high-quality knowledge graph extracted from software engineering code (Python and Java). It is designed for training the GraphMER-SE model for code understanding tasks.

## Dataset Contents

### Data Files
- \`enhanced_multilang.jsonl\` - Knowledge graph triples ($TRIPLE_COUNT entries)
- \`enhanced_multilang.entities.jsonl\` - Entity metadata and types

### Source Code
- \`src/\` - GraphMER model implementation
- \`configs/\` - Training configurations (CPU, GPU, TPU, Kaggle)
- \`scripts/\` - Training and evaluation scripts
- \`docs/specs/ontology_spec.yaml\` - Ontology specification

## Quick Start on Kaggle

### 1. Create New Notebook

1. Go to Kaggle Notebooks
2. Create new notebook
3. Settings โ Accelerator โ GPU โ Save
4. Add this dataset: Click "Add Data" โ Search "graphmer-kg"

### 2. Mount Dataset

\`\`\`python
import os
dataset_path = '/kaggle/input/graphmer-kg'
os.listdir(dataset_path)
\`\`\`

### 3. Run Training

\`\`\`python
# Copy source to working directory
!cp -r /kaggle/input/graphmer-kg/src /kaggle/working/
!cp -r /kaggle/input/graphmer-kg/configs /kaggle/working/
!cp -r /kaggle/input/graphmer-kg/scripts /kaggle/working/

# Install dependencies
!pip install transformers datasets pyyaml networkx

# Run training
!python /kaggle/working/scripts/train.py \\
    --config /kaggle/working/configs/train_kaggle.yaml \\
    --steps 10000
\`\`\`

For complete setup, see the included Jupyter notebook: \`GraphMER_Kaggle_Training.ipynb\`

## Data Format

### Triples Format (JSONL)
\`\`\`json
{
  "head": "entity_1",
  "relation": "inherits",
  "tail": "entity_2",
  "source": "file.py",
  "confidence": 1.0
}
\`\`\`

### Entity Format (JSONL)
\`\`\`json
{
  "entity_id": "entity_1",
  "type": "class",
  "metadata": {}
}
\`\`\`

## Validation Results

- **Total Triples:** $TRIPLE_COUNT
- **Domain/Range Compliance:** 99.10%
- **Acyclicity:** โ Verified (inheritance graph is acyclic)
- **Relation Coverage:** All 8 core relations present

## Hardware Requirements

### Kaggle GPU (Recommended)
- Tesla T4/P100 (16GB VRAM)
- Batch size: 64
- Training time: ~10-15 min for 10k steps

### Colab TPU
- TPU v2-8 (8 cores)
- Batch size: 16 (2 per core)
- Training time: ~15-20 min for 1k steps

### Local GPU
- RTX 3050 (8GB) or better
- Batch size: 32
- Training time: ~20-30 min for 10k steps

## License

CC0-1.0 (Public Domain)

## Citation

If you use this dataset, please cite:

\`\`\`bibtex
@dataset{graphmer_kg_2025,
  title={GraphMER Knowledge Graph Dataset},
  author={GraphMER Team},
  year={2025},
  publisher={Kaggle},
  version={1.0}
}
\`\`\`

## Support

For issues or questions:
- GitHub: [Repository URL]
- Kaggle: [Dataset Discussion]

---

**Last Updated:** $(date +%Y-%m-%d)
**Status:** โ Production Ready
EOF
echo "   โ Created README.md"

# Create requirements.txt
cat > "$OUTPUT_DIR/requirements.txt" <<EOF
torch>=2.0.0
transformers>=4.30.0
datasets>=2.12.0
pyyaml>=6.0
networkx>=3.1
tensorboard>=2.13.0
EOF
echo "   โ Created requirements.txt"
echo ""

# Step 6: Create archive
echo -e "${YELLOW}๐ฆ Step 6/6: Creating archive...${NC}"
cd "$PROJECT_ROOT"

# Try zip first, fall back to tar.gz
if command -v zip &> /dev/null; then
    ARCHIVE_NAME="graphmer_kaggle_dataset.zip"
    if [ -f "$ARCHIVE_NAME" ]; then
        rm "$ARCHIVE_NAME"
    fi
    cd "$OUTPUT_DIR"
    zip -r "../$ARCHIVE_NAME" . -x "*.git*" "*.DS_Store" "__pycache__/*" "*.pyc" > /dev/null
    cd "$PROJECT_ROOT"
    echo "   โ Created ZIP archive (web upload compatible)"
else
    ARCHIVE_NAME="graphmer_kaggle_dataset.tar.gz"
    if [ -f "$ARCHIVE_NAME" ]; then
        rm "$ARCHIVE_NAME"
    fi
    tar -czf "$ARCHIVE_NAME" -C "$OUTPUT_DIR" .
    echo "   โ Created TAR.GZ archive (extract and upload via web)"
    echo "   ๐ก Note: For web upload, extract this file first or upload via CLI"
fi

ARCHIVE_SIZE=$(du -h "$ARCHIVE_NAME" | cut -f1)
echo -e "   ${GREEN}โ Archive: $ARCHIVE_NAME ($ARCHIVE_SIZE)${NC}"
echo ""

# Final summary
echo -e "${GREEN}โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ${NC}"
echo -e "${GREEN}โ                                                              โ${NC}"
echo -e "${GREEN}โ                  DEPLOYMENT PACKAGE READY                    โ${NC}"
echo -e "${GREEN}โ                                                              โ${NC}"
echo -e "${GREEN}โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ${NC}"
echo ""
echo -e "${BLUE}๐ฆ Package Details:${NC}"
echo "   File: $ARCHIVE_NAME"
echo "   Size: $ARCHIVE_SIZE"
echo "   Triples: $TRIPLE_COUNT"
echo "   Location: $PROJECT_ROOT/$ARCHIVE_NAME"
echo ""

echo -e "${BLUE}๐ What's Inside:${NC}"
echo "   โ Knowledge graph ($TRIPLE_COUNT triples)"
echo "   โ Source code (src/)"
echo "   โ Training configs (CPU, GPU, TPU, Kaggle)"
echo "   โ Training scripts"
echo "   โ Ontology specification"
echo "   โ Dataset metadata"
echo "   โ README documentation"
echo ""

echo -e "${YELLOW}๐ค Next Steps - Upload to Kaggle:${NC}"
echo ""
echo "METHOD 1: Web Upload (Easiest, <500MB)"
echo "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ"
echo "1. Go to: https://www.kaggle.com/datasets"
echo "2. Click: 'New Dataset'"
echo "3. Upload: $ARCHIVE_NAME"
echo "4. Fill in: Title, description"
echo "5. Click: 'Create'"
echo ""

echo "METHOD 2: Kaggle CLI (For larger files or automation)"
echo "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ"
echo "# Install Kaggle CLI"
echo "pip install kaggle"
echo ""
echo "# Setup API token"
echo "# 1. Go to: https://www.kaggle.com/YOUR_USERNAME/account"
echo "# 2. Click: 'Create New API Token'"
echo "# 3. Save: kaggle.json to ~/.kaggle/"
echo "mkdir -p ~/.kaggle"
echo "mv ~/Downloads/kaggle.json ~/.kaggle/"
echo "chmod 600 ~/.kaggle/kaggle.json"
echo ""
echo "# Edit metadata (update YOUR_USERNAME)"
echo "nano $OUTPUT_DIR/dataset-metadata.json"
echo ""
echo "# Create dataset"
echo "cd $OUTPUT_DIR"
echo "kaggle datasets create -p ."
echo ""
echo "# Or update existing dataset"
echo "kaggle datasets version -p . -m 'Updated data'"
echo ""

echo -e "${YELLOW}๐ After Upload - Use in Notebook:${NC}"
echo "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ"
echo "1. Create new Kaggle notebook"
echo "2. Settings โ Accelerator โ GPU โ Save"
echo "3. Click 'Add Data' โ Search 'graphmer-kg'"
echo "4. Data mounts at: /kaggle/input/graphmer-kg/"
echo "5. Upload GraphMER_Kaggle_Training.ipynb or copy cells"
echo "6. Run training!"
echo ""

echo -e "${BLUE}๐ก Tips:${NC}"
echo "โข Dataset will be PRIVATE by default (you can make public later)"
echo "โข Max file size: 2GB via CLI, 500MB via web"
echo "โข Current size: $ARCHIVE_SIZE ($([ ${ARCHIVE_SIZE%M} -lt 500 ] 2>/dev/null && echo 'Web OK' || echo 'Use CLI'))"
echo "โข Update version: Use 'kaggle datasets version' command"
echo ""

echo -e "${GREEN}โ Deployment package created successfully!${NC}"
echo ""
